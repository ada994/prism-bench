<div align="center">

# PRISM Benchmark & FLUX-Reason-6M Dataset

[[ğŸŒ Homepage](https://flux-reason-6m.github.io/)] [[ğŸ¤— Huggingface Dataset](https://huggingface.co/datasets/LucasFang/FLUX-Reason-6M)] [[ğŸ“Š Leaderboard ](https://flux-reason-6m.github.io/)] [[ğŸ“Š Leaderboard-ZH ](https://flux-reason-6m.github.io/)] [[ğŸ“– Paper](https://flux-reason-6m.github.io/)]

[Rongyao Fang](https://rongyaofang.github.io/)<sup>1*</sup>&ensp; [Aldrich Yu](https://aldrichyu.github.io/)<sup>1*</sup>&ensp; [Chengqi Duan](https://scholar.google.com/citations?user=r9qb4ZwAAAAJ&hl=en)<sup>2*</sup>&ensp; [Linjiang Huang](https://leonhlj.github.io/)<sup>3</sup>&ensp; [Shuai Bai](https://scholar.google.com/citations?user=ylhI1JsAAAAJ&hl=zh-CN)<sup>4</sup>&ensp; 

[Yuxuan Cai](https://scholar.google.com/citations?user=EzYiBeUAAAAJ&hl=en)<sup>4</sup>&ensp; [Kun Wang](https://openreview.net/profile?id=~Kun_Wang8)&ensp; [Si Liu](https://scholar.google.com/citations?user=-QtVtNEAAAAJ&hl=en)<sup>3</sup>&ensp; [Xihui Liu](https://xh-liu.github.io/)<sup>2â€ </sup>&ensp; [Hongsheng Li](https://www.ee.cuhk.edu.hk/~hsli/)<sup>1â€ </sup>&ensp;

<sup>1</sup>CUHK&ensp;&ensp; <sup>2</sup>HKU&ensp;&ensp; <sup>3</sup>BUAA&ensp;&ensp; <sup>4</sup>Alibaba&ensp;&ensp; 

<sup>*</sup>Equal Contribution&ensp; <sup>â€ </sup>Corresponding Author

</div>

## ğŸ“– Introduction

ğŸŒŸ  This is the official repository for the paper "[FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark](https://flux-reason-6m.github.io/)", which contains both evaluation code and data for the **PRISM Benchmark**.

<p align="center">
  <img src="assets/teaser.png" alt="Teaser" width="1000"/>
</p>

We introduce **FLUX-Reason-6M** and **PRISM-Bench**. FLUX-Reason-6M is a **6-million-scale** synthesized dataset designed to incorporate reasoning capabilities into the architecture of T2I generation. PRISM-Bench serves as a comprehensive and discriminative benchmark with **7 independent tracks** that closely align with human judgment.

## ğŸ’¥ News
- **[2024-09-12]** Our paper is now accessible at [ArXiv Paper](https://flux-reason-6m.github.io/).
- **[2025-09-12]** Our FLUX-Reason-6M dataset is now accessible at [huggingface](https://huggingface.co/datasets/LucasFang/FLUX-Reason-6M).

## ğŸ“ˆ Evaluation

### Data
Please organize the image data as follows.
```sh
â””â”€â”€ images
â”‚Â Â  â”œâ”€â”€ imagination
â”‚   â”‚   â”œâ”€â”€ 0.png
â”‚   â”‚   â”œâ”€â”€ 1.png
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ 99.png
â”‚Â Â  â”œâ”€â”€ entity
â”‚Â Â  â”œâ”€â”€ text_rendering
â”‚Â Â  â”œâ”€â”€ style
â”‚Â Â  â”œâ”€â”€ affection
â”‚Â Â  â”œâ”€â”€ composition
â”‚Â Â  â”œâ”€â”€ long_text
```

### PRISM-Bench Evaluation

#### Eval with GPT4.1:

```sh
python evaluation/eval_gpt41.py --image_path <path to image data> --api_key <OpenAI API key> --base_url <OpenAI base URL for custom or proxy endpoints>
```
#### Eval with Qwen2.5-VL-72B:
```sh
python evaluation/eval_qwen25.py --image_path <path to image data> --model_path <path to qwen model> --output_dir <path to save results>
```

### PRISM-Bench-ZH Evaluation

#### Eval with GPT4.1:

```sh
python evaluation/eval_gpt41.py --image_path <path to image data> --api_key <OpenAI API key> --base_url <OpenAI base URL for custom or proxy endpoints> --zh
```
#### Eval with Qwen2.5-VL-72B:
```sh
python evaluation/eval_qwen25.py --image_path <path to image data> --model_path <path to qwen model> --output_dir <path to save results> --zh
```

## ğŸ“ Citation

If you find this benchmark and dataset useful in your research, please consider citing this BibTex:

```

```
